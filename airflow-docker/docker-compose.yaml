x-airflow-common: &airflow-common
  build: .
  environment: &airflow-common-env
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
    AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW_CONN_POSTGRES_DATA: ${AIRFLOW_CONN_POSTGRES_DATA}
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
    AIRFLOW_HOME: /opt/airflow
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    AIRFLOW__WEBSERVER__BASE_URL: ${AIRFLOW__WEBSERVER__BASE_URL}
    AIRFLOW__WEBSERVER__WEB_SERVER_HOST: ${AIRFLOW__WEBSERVER__WEB_SERVER_HOST}
    AIRFLOW__WEBSERVER__WEB_SERVER_PORT: ${AIRFLOW__WEBSERVER__WEB_SERVER_PORT}
    AIRFLOW__WEBSERVER__AUTHENTICATE: ${AIRFLOW__WEBSERVER__AUTHENTICATE}
    AIRFLOW__WEBSERVER__AUTH_BACKEND: ${AIRFLOW__WEBSERVER__AUTH_BACKEND}
    AIRFLOW__WEBSERVER__WORKER_CLASS: ${AIRFLOW__WEBSERVER__WORKER_CLASS}
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: ${AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL}
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: ${AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./requirements.txt:/opt/airflow/requirements.txt
  user: '${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}'
  command: bash -c "mkdir -p /opt/airflow/logs/dag_processor_manager && chown -R ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0} /opt/airflow/logs"
  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy
    postgres-data:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER}']
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: always
    ports:
      - '5433:5432'
    networks:
      - common-network

  postgres-data:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-data-volume:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER}']
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: always
    networks:
      - common-network

  airflow-init:
    build: .
    entrypoint: /bin/bash
    command: >
      -c "
      mkdir -p /opt/airflow/logs/dag_processor_manager &&
      chown -R airflow:root /opt/airflow &&
      su airflow -c 'python -m pip install --upgrade pip' &&
      su airflow -c 'PYTHONPATH=/opt/airflow pip install -r /opt/airflow/requirements.txt' &&
      su airflow -c 'airflow db init' &&
      su airflow -c 'airflow users create --username airflow --password airflow --firstname admin --lastname admin --role Admin --email admin@example.com'
      "
    environment:
      <<: *airflow-common-env
      PYTHONPATH: /opt/airflow
    user: '0:0'
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./requirements.txt:/opt/airflow/requirements.txt
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - common-network

  airflow-webserver:
    <<: *airflow-common
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.1}
    user: '0:0'
    command: bash -c "su airflow -c 'pip install -r /opt/airflow/requirements.txt' && mkdir -p /opt/airflow/logs/dag_processor_manager && chown -R airflow:root /opt/airflow/logs && chmod -R 777 /opt/airflow/logs && airflow webserver"
    ports:
      - '8080:8080'
    healthcheck:
      test: ['CMD', 'curl', '--fail', 'http://localhost:8080/health']
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - common-network
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./requirements.txt:/opt/airflow/requirements.txt

  airflow-scheduler:
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.1}
    user: '0:0'
    command: bash -c "su airflow -c 'pip install -r /opt/airflow/requirements.txt' && mkdir -p /opt/airflow/logs/dag_processor_manager && chown -R airflow:root /opt/airflow/logs && chmod -R 777 /opt/airflow/logs && airflow scheduler"
    environment:
      <<: *airflow-common-env
    healthcheck:
      test: ['CMD-SHELL', 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - common-network
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./requirements.txt:/opt/airflow/requirements.txt

  django:
    build:
      context: ./real_estate_api
      dockerfile: Dockerfile
    volumes:
      - django-static-volume:/app/staticfiles
    ports:
      - '8000:8000'
    environment:
      - DJANGO_SETTINGS_MODULE=real_estate_api.settings
      - DB_HOST=postgres-data
      - DB_PORT=5432
      - DB_NAME=realestate
      - DB_USER=realestate
      - DB_PASSWORD=realestate
      - DEBUG=1
      - ALLOWED_HOSTS=localhost
    depends_on:
      postgres-data:
        condition: service_healthy
    restart: always
    networks:
      - common-network

  tor:
    image: dperson/torproxy
    ports:
      - '9050:9050'
    restart: always
    networks:
      - common-network

volumes:
  postgres-db-volume:
  postgres-data-volume:
  django-static-volume:

networks:
  common-network:
    driver: bridge
